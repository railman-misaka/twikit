{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOknoFQDFOPFQqSxTCK7HGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/railman-misaka/twikit/blob/main/%E3%80%90%E6%94%B9%E4%BF%AE_ReBest%E6%A7%98%E3%80%91241231twikit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# セットアップ"
      ],
      "metadata": {
        "id": "fqrJ-q5N75ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "①セットアップ"
      ],
      "metadata": {
        "id": "VU-xKaj59QZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dFf1_Gm39PDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインストール\n",
        "!pip install twikit pandas openpyxl\n",
        "\n",
        "# 必要なインポート\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from twikit import Client\n",
        "import asyncio\n",
        "from collections import Counter\n",
        "from google.colab import output\n",
        "\n",
        "# Google Driveのマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ディレクトリ設定\n",
        "WORK_DIR = '/content/drive/MyDrive/Twitter_Analysis'\n",
        "COOKIE_DIR = f\"{WORK_DIR}/twitter_json\"\n",
        "RESULTS_DIR = f\"{WORK_DIR}/profile_results\"\n",
        "\n",
        "# ディレクトリ作成\n",
        "os.makedirs(COOKIE_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"セットアップ完了！\")\n",
        "print(f\"Cookie path: {COOKIE_DIR}/cookie_edit.json\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-D8J4b_sHug",
        "outputId": "fe72d3bd-7804-4a04-854f-37e1b962ae64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twikit\n",
            "  Downloading twikit-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: httpx[socks] in /usr/local/lib/python3.10/dist-packages (from twikit) (0.28.1)\n",
            "Collecting filetype (from twikit)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from twikit) (4.12.3)\n",
            "Collecting pyotp (from twikit)\n",
            "  Downloading pyotp-2.9.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from twikit) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->twikit) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (3.10)\n",
            "Collecting socksio==1.* (from httpx[socks]->twikit)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[socks]->twikit) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[socks]->twikit) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[socks]->twikit) (1.2.2)\n",
            "Downloading twikit-2.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading pyotp-2.9.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: filetype, socksio, pyotp, twikit\n",
            "Successfully installed filetype-1.2.0 pyotp-2.9.0 socksio-1.0.0 twikit-2.2.0\n",
            "Mounted at /content/drive\n",
            "セットアップ完了！\n",
            "Cookie path: /content/drive/MyDrive/Twitter_Analysis/twitter_json/cookie_edit.json\n",
            "Results directory: /content/drive/MyDrive/Twitter_Analysis/profile_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "②# クッキーファイルのアップロード X連携"
      ],
      "metadata": {
        "id": "WIH2gJ-C8plQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# リプライサーチ"
      ],
      "metadata": {
        "id": "9XQxmFYG9Z6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# クッキーファイルのアップロード\n",
        "def upload_cookie_file():\n",
        "    print(\"cookie_edit.jsonファイルをアップロードしてください\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if 'cookie_edit.json' in uploaded:\n",
        "        with open(f\"{COOKIE_DIR}/cookie_edit.json\", 'wb') as f:\n",
        "            f.write(uploaded['cookie_edit.json'])\n",
        "        print(\"クッキーファイルを保存しました\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"cookie_edit.jsonファイルがアップロードされませんでした\")\n",
        "        return False\n",
        "\n",
        "upload_cookie_file()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "bnkVy3lIwJ4_",
        "outputId": "5760fe78-7443-495e-ad07-2574cbc544cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cookie_edit.jsonファイルをアップロードしてください\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3896a40-a1be-4fcb-bf7a-553a552e7af9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3896a40-a1be-4fcb-bf7a-553a552e7af9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cookie_edit.json to cookie_edit.json\n",
            "クッキーファイルを保存しました\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "③初期実行　リプライサーチ"
      ],
      "metadata": {
        "id": "y-lPYe5u8vzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TwitterProfileAnalyzerクラスの定義\n",
        "class TwitterProfileAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.client = Client(language='en-US')\n",
        "        self.cookie_path = f\"{COOKIE_DIR}/cookie_edit.json\"\n",
        "        self.results_dir = RESULTS_DIR\n",
        "\n",
        "    async def setup(self):\n",
        "        try:\n",
        "            with open(self.cookie_path, 'r', encoding='utf-8') as file:\n",
        "                cookies = json.load(file)\n",
        "            self.client.set_cookies(cookies)\n",
        "            print(\"認証に成功しました！\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"認証エラー: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def analyze_user_replies(self, screen_name, tweets_to_analyze=200):\n",
        "        \"\"\"指定したユーザーのツイートから、リプライを分析する\"\"\"\n",
        "        try:\n",
        "            # ユーザー情報を取得\n",
        "            target_user = await self.client.get_user_by_screen_name(screen_name)\n",
        "            print(f\"{screen_name}のツイートを分析中...\")\n",
        "\n",
        "            # リプライしているユーザーをスクリーンネームベースで追跡\n",
        "            reply_counter = Counter()\n",
        "            reply_users = {}\n",
        "\n",
        "            # ツイートを取得（リプライを含む）\n",
        "            print(\"ツイートを取得中...\")\n",
        "            results = await self.client.get_user_tweets(\n",
        "                target_user.id,\n",
        "                tweet_type='Replies',  # Repliesタイプに変更\n",
        "                count=min(tweets_to_analyze, 100)\n",
        "            )\n",
        "\n",
        "            analyzed_count = 0\n",
        "            while results and analyzed_count < tweets_to_analyze:\n",
        "                for tweet in results:\n",
        "                    try:\n",
        "                        # リプライ先のツイートテキストを解析\n",
        "                        if hasattr(tweet, 'text') and tweet.text.startswith('@'):\n",
        "                            # @ユーザー名を抽出\n",
        "                            mentioned_users = [\n",
        "                                word[1:] for word in tweet.text.split()\n",
        "                                if word.startswith('@')\n",
        "                            ]\n",
        "\n",
        "                            for reply_to in mentioned_users:\n",
        "                                if reply_to and reply_to != screen_name:\n",
        "                                    try:\n",
        "                                        if reply_to not in reply_users:\n",
        "                                            # スクリーンネームからユーザー情報を取得\n",
        "                                            try:\n",
        "                                                reply_user = await self.client.get_user_by_screen_name(reply_to)\n",
        "                                                reply_users[reply_to] = reply_user\n",
        "                                                print(f\"ユーザー {reply_to} の情報を取得しました\")\n",
        "                                            except Exception as e:\n",
        "                                                continue\n",
        "                                        reply_counter[reply_to] += 1\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"ユーザー {reply_to} の情報取得をスキップ: {e}\")\n",
        "                                        continue\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                    analyzed_count += 1\n",
        "                    if analyzed_count % 20 == 0:\n",
        "                        print(f\"{analyzed_count}件のツイートを分析済み\")\n",
        "\n",
        "                if analyzed_count < tweets_to_analyze and results.next_cursor:\n",
        "                    try:\n",
        "                        results = await results.next()\n",
        "                    except Exception as e:\n",
        "                        print(f\"追加ツイート取得エラー: {e}\")\n",
        "                        break\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            print(f\"\\n分析完了: {analyzed_count}件のツイートを処理\")\n",
        "            print(f\"リプライ先ユーザー数: {len(reply_users)}人\")\n",
        "\n",
        "            return reply_counter, reply_users\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"分析エラー: {e}\")\n",
        "            return Counter(), {}\n",
        "\n",
        "    async def get_user_profile(self, user):\n",
        "        \"\"\"ユーザーのプロフィール情報を取得する\"\"\"\n",
        "        try:\n",
        "            # プロフィール情報を辞書形式で整理\n",
        "            profile_data = {\n",
        "                'user_id': user.id,                      # ユーザーID\n",
        "                'name': user.name,                       # 表示名\n",
        "                'screen_name': user.screen_name,         # @ユーザー名\n",
        "                'description': user.description,         # プロフィール文\n",
        "                'location': user.location,               # 場所\n",
        "                'followers_count': user.followers_count, # フォロワー数\n",
        "                'following_count': user.following_count, # フォロー数\n",
        "                'tweets_count': user.statuses_count,    # ツイート数\n",
        "                'created_at': user.created_at,          # アカウント作成日\n",
        "                'profile_image_url': user.profile_image_url  # プロフィール画像URL\n",
        "            }\n",
        "            return profile_data\n",
        "        except Exception as e:\n",
        "            print(f\"プロフィール取得エラー: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def get_user_tweets(self, user, count=1):\n",
        "        \"\"\"ユーザーの投稿を取得する\"\"\"\n",
        "        try:\n",
        "            tweets = []\n",
        "            results = await user.get_tweets(tweet_type='Tweets', count=count)\n",
        "\n",
        "            for tweet in results:\n",
        "                tweet_data = {\n",
        "                    'tweet_id': tweet.id,                # ツイートID\n",
        "                    'text': tweet.text,                  # ツイート本文\n",
        "                    'created_at': tweet.created_at,      # 投稿日時\n",
        "                    'retweet_count': tweet.retweet_count, # リツイート数\n",
        "                    'like_count': tweet.favorite_count,   # いいね数\n",
        "                    'reply_count': tweet.reply_count,     # 返信数\n",
        "                    'is_retweet': bool(tweet.retweeted_tweet), # リツイートかどうか\n",
        "                    'is_quote': tweet.is_quote_status     # 引用ツイートかどうか\n",
        "                }\n",
        "                tweets.append(tweet_data)\n",
        "\n",
        "            return tweets\n",
        "        except Exception as e:\n",
        "            print(f\"ツイート取得エラー: {e}\")\n",
        "            return []\n",
        "\n",
        "    def save_results(self, frequent_repliers_data, target_screen_name):\n",
        "        \"\"\"分析結果をJSONファイルとして保存する\"\"\"\n",
        "        try:\n",
        "            current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            filename = os.path.join(\n",
        "                self.results_dir,\n",
        "                f\"analysis_{target_screen_name}_{current_time}.json\"\n",
        "            )\n",
        "\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(frequent_repliers_data, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"分析結果を保存しました: {filename}\")\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            print(f\"保存エラー: {e}\")\n",
        "            return None\n",
        "\n",
        "    def save_to_excel(self, frequent_repliers_data, target_screen_name):\n",
        "        \"\"\"分析結果をExcelファイルとして保存\"\"\"\n",
        "        try:\n",
        "            # データフレーム用のリストを作成\n",
        "            rows = []\n",
        "            for user_data in frequent_repliers_data:\n",
        "                profile = user_data['profile']\n",
        "                tweets = user_data['recent_tweets']\n",
        "\n",
        "                # 最新のツイート3件を結合\n",
        "                recent_tweets_text = \"\\n\".join(\n",
        "                    [tweet['text'] for tweet in tweets[:3]]\n",
        "                ) if tweets else \"\"\n",
        "\n",
        "                # 1行のデータとして整形\n",
        "                row = {\n",
        "                    '分析対象ユーザー': target_screen_name,\n",
        "                    '分析日時': datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
        "                    'ユーザー名': profile['name'],\n",
        "                    'ユーザーID': f\"@{profile['screen_name']}\",\n",
        "                    'プロフィール文': profile['description'],\n",
        "                    'アカウント作成日': profile['created_at'],\n",
        "                    'リプライ数': user_data['reply_count'],\n",
        "                    'フォロワー数': profile['followers_count'],\n",
        "                    'フォロー数': profile['following_count'],\n",
        "                    'ツイート数': profile['tweets_count'],\n",
        "                    '場所': profile['location'],\n",
        "                    'プロフィール画像URL': profile['profile_image_url'],\n",
        "                    'アカウントURL': f\"https://twitter.com/{profile['screen_name']}\",\n",
        "                    '最近のツイート': recent_tweets_text\n",
        "                }\n",
        "                rows.append(row)\n",
        "\n",
        "            # DataFrameを作成\n",
        "            df = pd.DataFrame(rows)\n",
        "\n",
        "             # profile_resultsディレクトリを作成（存在しない場合）\n",
        "            profile_results_dir = \"profile_results\"\n",
        "            os.makedirs(profile_results_dir, exist_ok=True)\n",
        "\n",
        "            # Excelファイル名を生成\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            excel_file = f\"{self.results_dir}/リプライ分析_{target_screen_name}_{timestamp}.xlsx\"\n",
        "\n",
        "            # Excelファイルとして保存\n",
        "            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "                df.to_excel(writer, sheet_name='リプライ分析結果', index=False)\n",
        "\n",
        "                # 列幅の自動調整\n",
        "                worksheet = writer.sheets['リプライ分析結果']\n",
        "                for idx, col in enumerate(df.columns):\n",
        "                    max_length = max(\n",
        "                        df[col].astype(str).apply(len).max(),\n",
        "                        len(str(col))\n",
        "                    )\n",
        "                    # 最大幅を50文字に制限\n",
        "                    adjusted_width = min(max_length + 2, 50)\n",
        "                    worksheet.column_dimensions[chr(65 + idx)].width = adjusted_width\n",
        "\n",
        "                # 行の高さを調整（最近のツイート用）\n",
        "                for row in range(2, len(df) + 2):  # Excelは1始まりでヘッダーがあるため+2\n",
        "                    worksheet.row_dimensions[row].height = 60\n",
        "\n",
        "            print(f\"\\nExcelファイルを保存しました: {excel_file}\")\n",
        "            return excel_file\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Excelファイルの保存中にエラーが発生しました: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    async def run_analysis():\n",
        "        analyzer = TwitterProfileAnalyzer()\n",
        "\n",
        "        if not await analyzer.setup():\n",
        "            return\n",
        "\n",
        "        # ユーザー入力を対話的に取得\n",
        "        target_user = input(\"分析対象のユーザー名を入力してください（@を除く）: \")\n",
        "        min_replies = int(input(\"最小リプライ数を入力してください（例: 3）: \"))\n",
        "        tweets_to_analyze = int(input(\"分析するツイート数を入力してください（例: 200）: \"))\n",
        "\n",
        "        print(f\"\\n{target_user}のリプライを分析します...\")\n",
        "        print(f\"- 分析対象ツイート数: {tweets_to_analyze}\")\n",
        "        print(f\"- 最小リプライ数: {min_replies}\")\n",
        "\n",
        "        # リプライを分析\n",
        "        reply_counter, reply_users = await analyzer.analyze_user_replies(target_user, tweets_to_analyze)\n",
        "\n",
        "        if not reply_counter:\n",
        "            print(\"\\nリプライが見つかりませんでした。\")\n",
        "            return\n",
        "\n",
        "        # 頻繁にリプライしているユーザーの情報を収集\n",
        "        frequent_repliers_data = []\n",
        "        print(\"\\nリプライの多いユーザーの情報を収集中...\")\n",
        "\n",
        "        for screen_name, reply_count in reply_counter.most_common():\n",
        "            if reply_count >= min_replies and screen_name in reply_users:\n",
        "                try:\n",
        "                    user = reply_users[screen_name]\n",
        "                    profile_data = await analyzer.get_user_profile(user)\n",
        "                    tweets = await analyzer.get_user_tweets(user, count=3)\n",
        "\n",
        "                    if profile_data:\n",
        "                        user_data = {\n",
        "                            'profile': profile_data,\n",
        "                            'reply_count': reply_count,\n",
        "                            'recent_tweets': tweets\n",
        "                        }\n",
        "                        frequent_repliers_data.append(user_data)\n",
        "                        print(f\"@{screen_name}の情報を取得しました（リプライ数: {reply_count}）\")\n",
        "                except Exception as e:\n",
        "                    print(f\"@{screen_name}の情報取得に失敗: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # 結果を保存\n",
        "        if frequent_repliers_data:\n",
        "            analyzer.save_results(frequent_repliers_data, target_user)\n",
        "            analyzer.save_to_excel(frequent_repliers_data, target_user)\n",
        "            print(\"\\n分析が完了しました！\")\n",
        "        else:\n",
        "            print(\"\\n分析対象となるユーザーが見つかりませんでした。\")\n",
        "\n",
        "# Google Colab用の非同期\n",
        "\n",
        "# Google Colab用の実行コード\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# 分析の実行\n",
        "await run_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7cbSGtBruWa",
        "outputId": "869719fd-374d-479f-c9c1-a60da25c6342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "認証に成功しました！\n",
            "分析対象のユーザー名を入力してください（@を除く）: sora19ai\n",
            "最小リプライ数を入力してください（例: 3）: 3\n",
            "分析するツイート数を入力してください（例: 200）: 100\n",
            "\n",
            "sora19aiのリプライを分析します...\n",
            "- 分析対象ツイート数: 100\n",
            "- 最小リプライ数: 3\n",
            "sora19aiのツイートを分析中...\n",
            "ツイートを取得中...\n",
            "ユーザー 1108k4xi の情報を取得しました\n",
            "20件のツイートを分析済み\n",
            "ユーザー AquaCleanWater の情報を取得しました\n",
            "ユーザー kawa11japan の情報を取得しました\n",
            "40件のツイートを分析済み\n",
            "60件のツイートを分析済み\n",
            "80件のツイートを分析済み\n",
            "ユーザー solt152152 の情報を取得しました\n",
            "100件のツイートを分析済み\n",
            "\n",
            "分析完了: 111件のツイートを処理\n",
            "リプライ先ユーザー数: 4人\n",
            "\n",
            "リプライの多いユーザーの情報を収集中...\n",
            "\n",
            "分析対象となるユーザーが見つかりませんでした。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "④2回目以降　実行用　リプライサーチ"
      ],
      "metadata": {
        "id": "kkg-r00T2d7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_analysis():\n",
        "        analyzer = TwitterProfileAnalyzer()\n",
        "\n",
        "        if not await analyzer.setup():\n",
        "            return\n",
        "\n",
        "        # ユーザー入力を対話的に取得\n",
        "        target_user = input(\"分析対象のユーザー名を入力してください（@を除く）: \")\n",
        "        min_replies = int(input(\"最小リプライ数を入力してください（例: 3）: \"))\n",
        "        tweets_to_analyze = int(input(\"分析するツイート数を入力してください（例: 200）: \"))\n",
        "\n",
        "        print(f\"\\n{target_user}のリプライを分析します...\")\n",
        "        print(f\"- 分析対象ツイート数: {tweets_to_analyze}\")\n",
        "        print(f\"- 最小リプライ数: {min_replies}\")\n",
        "\n",
        "        # リプライを分析\n",
        "        reply_counter, reply_users = await analyzer.analyze_user_replies(target_user, tweets_to_analyze)\n",
        "\n",
        "        if not reply_counter:\n",
        "            print(\"\\nリプライが見つかりませんでした。\")\n",
        "            return\n",
        "\n",
        "        # 頻繁にリプライしているユーザーの情報を収集\n",
        "        frequent_repliers_data = []\n",
        "        print(\"\\nリプライの多いユーザーの情報を収集中...\")\n",
        "\n",
        "        for screen_name, reply_count in reply_counter.most_common():\n",
        "            if reply_count >= min_replies and screen_name in reply_users:\n",
        "                try:\n",
        "                    user = reply_users[screen_name]\n",
        "                    profile_data = await analyzer.get_user_profile(user)\n",
        "                    tweets = await analyzer.get_user_tweets(user, count=3)\n",
        "\n",
        "                    if profile_data:\n",
        "                        user_data = {\n",
        "                            'profile': profile_data,\n",
        "                            'reply_count': reply_count,\n",
        "                            'recent_tweets': tweets\n",
        "                        }\n",
        "                        frequent_repliers_data.append(user_data)\n",
        "                        print(f\"@{screen_name}の情報を取得しました（リプライ数: {reply_count}）\")\n",
        "                except Exception as e:\n",
        "                    print(f\"@{screen_name}の情報取得に失敗: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # 結果を保存\n",
        "        if frequent_repliers_data:\n",
        "            analyzer.save_results(frequent_repliers_data, target_user)\n",
        "            analyzer.save_to_excel(frequent_repliers_data, target_user)\n",
        "            print(\"\\n分析が完了しました！\")\n",
        "        else:\n",
        "            print(\"\\n分析対象となるユーザーが見つかりませんでした。\")\n",
        "\n",
        "# Google Colab用の非同期\n",
        "\n",
        "# Google Colab用の実行コード\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# 分析の実行\n",
        "await run_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQL4NpUQ2cul",
        "outputId": "ff42cdaa-5133-48a4-a6a3-a12a68902f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "認証に成功しました！\n",
            "分析対象のユーザー名を入力してください（@を除く）: sora19ai\n",
            "最小リプライ数を入力してください（例: 3）: 2\n",
            "分析するツイート数を入力してください（例: 200）: 200\n",
            "\n",
            "sora19aiのリプライを分析します...\n",
            "- 分析対象ツイート数: 200\n",
            "- 最小リプライ数: 2\n",
            "sora19aiのツイートを分析中...\n",
            "ツイートを取得中...\n",
            "ユーザー 1108k4xi の情報を取得しました\n",
            "20件のツイートを分析済み\n",
            "ユーザー AquaCleanWater の情報を取得しました\n",
            "ユーザー kawa11japan の情報を取得しました\n",
            "40件のツイートを分析済み\n",
            "60件のツイートを分析済み\n",
            "80件のツイートを分析済み\n",
            "ユーザー solt152152 の情報を取得しました\n",
            "100件のツイートを分析済み\n",
            "120件のツイートを分析済み\n",
            "140件のツイートを分析済み\n",
            "160件のツイートを分析済み\n",
            "180件のツイートを分析済み\n",
            "200件のツイートを分析済み\n",
            "\n",
            "分析完了: 209件のツイートを処理\n",
            "リプライ先ユーザー数: 4人\n",
            "\n",
            "リプライの多いユーザーの情報を収集中...\n",
            "@1108k4xiの情報を取得しました（リプライ数: 2）\n",
            "分析結果を保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/analysis_sora19ai_20241229_220112.json\n",
            "\n",
            "Excelファイルを保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/リプライ分析_sora19ai_20241229_220112.xlsx\n",
            "\n",
            "分析が完了しました！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 非同期で分析を実行する関数\n",
        "async def run_analysis():\n",
        "        # TwitterProfileAnalyzerクラスのインスタンスを作成\n",
        "        analyzer = TwitterProfileAnalyzer()\n",
        "\n",
        "        # セットアップを実行し、認証が成功したか確認\n",
        "        if not await analyzer.setup():\n",
        "            return  # 認証に失敗した場合は関数を終了\n",
        "\n",
        "        # ユーザー入力を対話的に取得\n",
        "        target_user = input(\"分析対象のユーザー名を入力してください（@を除く）: \")  # 分析対象のTwitterユーザー名を入力\n",
        "        min_replies = int(input(\"最小リプライ数を入力してください（例: 3）: \"))  # リプライの最小数を入力\n",
        "        tweets_to_analyze = int(input(\"分析するツイート数を入力してください（例: 200）: \"))  # 分析するツイートの数を入力\n",
        "\n",
        "        # 分析の開始をユーザーに通知\n",
        "        print(f\"\\n{target_user}のリプライを分析します...\")\n",
        "        print(f\"- 分析対象ツイート数: {tweets_to_analyze}\")\n",
        "        print(f\"- 最小リプライ数: {min_replies}\")\n",
        "\n",
        "        # 指定したユーザーのリプライを分析\n",
        "        reply_counter, reply_users = await analyzer.analyze_user_replies(target_user, tweets_to_analyze)\n",
        "\n",
        "        # リプライが見つからなかった場合の処理\n",
        "        if not reply_counter:\n",
        "            print(\"\\nリプライが見つかりませんでした。\")\n",
        "            return  # 関数を終了\n",
        "\n",
        "        # 頻繁にリプライしているユーザーの情報を収集するためのリストを初期化\n",
        "        frequent_repliers_data = []\n",
        "        print(\"\\nリプライの多いユーザーの情報を収集中...\")\n",
        "\n",
        "        # リプライカウントが多い順にユーザーを処理\n",
        "        for screen_name, reply_count in reply_counter.most_common():\n",
        "            # 最小リプライ数以上かつユーザー情報が存在する場合に処理\n",
        "            if reply_count >= min_replies and screen_name in reply_users:\n",
        "                try:\n",
        "                    # リプライ先のユーザーオブジェクトを取得\n",
        "                    user = reply_users[screen_name]\n",
        "                    # ユーザーのプロフィール情報を取得\n",
        "                    profile_data = await analyzer.get_user_profile(user)\n",
        "                    # ユーザーの最新3件のツイートを取得\n",
        "                    tweets = await analyzer.get_user_tweets(user, count=3)\n",
        "\n",
        "                    # プロフィール情報が取得できた場合にデータを構築\n",
        "                    if profile_data:\n",
        "                        user_data = {\n",
        "                            'profile': profile_data,        # ユーザープロフィール情報\n",
        "                            'reply_count': reply_count,     # リプライ数\n",
        "                            'recent_tweets': tweets         # 最近のツイート\n",
        "                        }\n",
        "                        # 頻繁なリプライユーザーのリストに追加\n",
        "                        frequent_repliers_data.append(user_data)\n",
        "                        # 情報取得完了を通知\n",
        "                        print(f\"@{screen_name}の情報を取得しました（リプライ数: {reply_count}）\")\n",
        "                except Exception as e:\n",
        "                    # ユーザー情報の取得に失敗した場合のエラーメッセージ\n",
        "                    print(f\"@{screen_name}の情報取得に失敗: {e}\")\n",
        "                    continue  # 次のユーザーに進む\n",
        "\n",
        "        # 収集したデータが存在する場合に結果を保存\n",
        "        if frequent_repliers_data:\n",
        "            # JSONファイルとして保存\n",
        "            analyzer.save_results(frequent_repliers_data, target_user)\n",
        "            # Excelファイルとして保存\n",
        "            analyzer.save_to_excel(frequent_repliers_data, target_user)\n",
        "            # 分析完了を通知\n",
        "            print(\"\\n分析が完了しました！\")\n",
        "        else:\n",
        "            # 分析対象のユーザーが見つからなかった場合のメッセージ\n",
        "            print(\"\\n分析対象となるユーザーが見つかりませんでした。\")\n",
        "\n",
        "# Google Colab用の非同期\n",
        "\n",
        "# Google Colab用の実行コード\n",
        "output.enable_custom_widget_manager()  # Google Colabでカスタムウィジェットを有効にする\n",
        "\n",
        "# 分析の実行\n",
        "await run_analysis()  # run_analysis関数を非同期で実行\n"
      ],
      "metadata": {
        "id": "ePHd-qq314iD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "f80ced26-1aa3-48b0-aabc-6672b7df9105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "認証に成功しました！\n",
            "分析対象のユーザー名を入力してください（@を除く）: sora19ai\n",
            "最小リプライ数を入力してください（例: 3）: 2\n",
            "分析するツイート数を入力してください（例: 200）: 200\n",
            "\n",
            "sora19aiのリプライを分析します...\n",
            "- 分析対象ツイート数: 200\n",
            "- 最小リプライ数: 2\n",
            "sora19aiのツイートを分析中...\n",
            "ツイートを取得中...\n",
            "ユーザー 1108k4xi の情報を取得しました\n",
            "20件のツイートを分析済み\n",
            "ユーザー AquaCleanWater の情報を取得しました\n",
            "ユーザー kawa11japan の情報を取得しました\n",
            "40件のツイートを分析済み\n",
            "60件のツイートを分析済み\n",
            "80件のツイートを分析済み\n",
            "ユーザー solt152152 の情報を取得しました\n",
            "100件のツイートを分析済み\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-956c228d89db>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# 分析の実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mrun_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run_analysis関数を非同期で実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-956c228d89db>\u001b[0m in \u001b[0;36mrun_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 指定したユーザーのリプライを分析\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mreply_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_user_replies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_to_analyze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# リプライが見つからなかった場合の処理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f15b23c13b86>\u001b[0m in \u001b[0;36manalyze_user_replies\u001b[0;34m(self, screen_name, tweets_to_analyze)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0manalyzed_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtweets_to_analyze\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_cursor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"追加ツイート取得エラー: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twikit/utils.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fetch_next_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fetch_next_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprevious\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twikit/client/client.py\u001b[0m in \u001b[0;36mget_user_tweets\u001b[0;34m(self, user_id, tweet_type, count, cursor)\u001b[0m\n\u001b[1;32m   1852\u001b[0m             \u001b[0;34m'Likes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_likes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         }[tweet_type]\n\u001b[0;32m-> 1854\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0minstructions_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'instructions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twikit/client/gql.py\u001b[0m in \u001b[0;36muser_tweets_and_replies\u001b[0;34m(self, user_id, count, cursor)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muser_tweets_and_replies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_user_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER_TWEETS_AND_REPLIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muser_media\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twikit/client/gql.py\u001b[0m in \u001b[0;36m_get_user_tweets\u001b[0;34m(self, user_id, count, cursor, endpoint)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cursor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgql_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muser_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twikit/client/gql.py\u001b[0m in \u001b[0;36mgql_get\u001b[0;34m(self, url, variables, features, headers, extra_params, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     async def gql_post(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twikit/client/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;34m':meta private:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twikit/client/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, auto_unlock, raise_exception, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mcookies_backup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_duplicate_ct0_cookie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         )\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0masynccontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         response = await self._send_handling_auth(\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                 response = await self._send_handling_redirects(\n\u001b[0m\u001b[1;32m   1658\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = await connection.handle_async_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAsyncNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = await self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = await self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sslobject_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36m_call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    136\u001b[0m                         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             ):\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/locks.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⑤2回目以降　実行用　リプライサーチ（X自動化のリプライ検索機能（相手から返信3回以上判定））"
      ],
      "metadata": {
        "id": "4mt09i4Ua42K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 非同期で分析を実行する関数\n",
        "async def run_analysis():\n",
        "        # TwitterProfileAnalyzerクラスのインスタンスを作成\n",
        "        analyzer = TwitterProfileAnalyzer()\n",
        "\n",
        "        # セットアップを実行し、認証が成功したか確認\n",
        "        if not await analyzer.setup():\n",
        "            return  # 認証に失敗した場合は関数を終了\n",
        "\n",
        "        # ユーザー入力を対話的に取得\n",
        "        target_user = input(\"分析対象のユーザー名を入力してください（@を除く）: \")  # 分析対象のTwitterユーザー名を入力\n",
        "        min_replies = int(input(\"最小リプライ数を入力してください（例: 3）: \"))  # リプライの最小数を入力\n",
        "        tweets_to_analyze = int(input(\"分析するツイート数を入力してください（例: 200）: \"))  # 分析するツイートの数を入力\n",
        "\n",
        "        # 分析の開始をユーザーに通知\n",
        "        print(f\"\\n{target_user}のリプライを分析します...\")\n",
        "        print(f\"- 分析対象ツイート数: {tweets_to_analyze}\")\n",
        "        print(f\"- 最小リプライ数: {min_replies}\")\n",
        "\n",
        "        # 指定したユーザーのリプライを分析\n",
        "        reply_counter, reply_users = await analyzer.analyze_user_replies(target_user, tweets_to_analyze)\n",
        "\n",
        "        # リプライが見つからなかった場合の処理\n",
        "        if not reply_counter:\n",
        "            print(\"\\nリプライが見つかりませんでした。\")\n",
        "            return  # 関数を終了\n",
        "\n",
        "        # 頻繁にリプライしているユーザーの情報を収集するためのリストを初期化\n",
        "        frequent_repliers_data = []\n",
        "        print(\"\\nリプライの多いユーザーの情報を収集中...\")\n",
        "\n",
        "        # リプライカウントが多い順にユーザーを処理\n",
        "        for screen_name, reply_count in reply_counter.most_common():\n",
        "            # 最小リプライ数以上かつユーザー情報が存在する場合に処理\n",
        "            if reply_count >= min_replies and screen_name in reply_users:\n",
        "                try:\n",
        "                    # リプライ先のユーザーオブジェクトを取得\n",
        "                    user = reply_users[screen_name]\n",
        "                    # ユーザーのプロフィール情報を取得\n",
        "                    profile_data = await analyzer.get_user_profile(user)\n",
        "                    # ユーザーの最新3件のツイートを取得\n",
        "                    tweets = await analyzer.get_user_tweets(user, count=3)\n",
        "\n",
        "                    # 相手からのリプライを取得（Twikitを使用）\n",
        "                    received_replies = []\n",
        "                    try:\n",
        "                        # 検索クエリを構成\n",
        "                        query = f\"from:{screen_name} to:{target_user}\"\n",
        "                        # Twikitのsearch_tweet関数を使用\n",
        "                        search_results = await analyzer.client.search_tweet(query, \"Latest\", count=20)\n",
        "\n",
        "                        for tweet in search_results:\n",
        "                            # ツイートがリプライであることを確認\n",
        "                            tweet_data = tweet.__dict__.get('data', {})\n",
        "                            referenced_tweets = tweet_data.get('referenced_tweets', [])\n",
        "\n",
        "                            is_reply = any(ref.get('type') == 'replied_to' for ref in referenced_tweets)\n",
        "                            if is_reply:\n",
        "                                received_replies.append({\n",
        "                                    'text': tweet.text,\n",
        "                                    'created_at': tweet.created_at,\n",
        "                                    'id': tweet.id\n",
        "                                })\n",
        "\n",
        "                            if len(received_replies) >= 3:  # 最大3件まで取得\n",
        "                                break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"リプライの取得中にエラーが発生: {e}\")\n",
        "                        received_replies = []\n",
        "\n",
        "                    # プロフィール情報が取得できた場合にデータを構築\n",
        "                    if profile_data:\n",
        "                        user_data = {\n",
        "                            'profile': profile_data,            # ユーザープロフィール情報\n",
        "                            'reply_count': reply_count,         # リプライ数\n",
        "                            'recent_tweets': tweets,            # 最近のツイート\n",
        "                            'received_replies': received_replies  # 相手からのリプライ（新機能）\n",
        "                        }\n",
        "                        # 頻繁なリプライユーザーのリストに追加\n",
        "                        frequent_repliers_data.append(user_data)\n",
        "                        # 情報取得完了を通知\n",
        "                        print(f\"@{screen_name}の情報を取得しました（リプライ数: {reply_count}）\")\n",
        "                except Exception as e:\n",
        "                    # ユーザー情報の取得に失敗した場合のエラーメッセージ\n",
        "                    print(f\"@{screen_name}の情報取得に失敗: {e}\")\n",
        "                    continue  # 次のユーザーに進む\n",
        "\n",
        "        # 収集したデータが存在する場合に結果を保存\n",
        "        if frequent_repliers_data:\n",
        "            # JSONファイルとして保存\n",
        "            analyzer.save_results(frequent_repliers_data, target_user)\n",
        "            # Excelファイルとして保存\n",
        "            analyzer.save_to_excel(frequent_repliers_data, target_user)\n",
        "            # 分析完了を通知\n",
        "            print(\"\\n分析が完了しました！\")\n",
        "        else:\n",
        "            # 分析対象のユーザーが見つからなかった場合のメッセージ\n",
        "            print(\"\\n分析対象となるユーザーが見つかりませんでした。\")\n",
        "\n",
        "# Excelファイルに保存する関数\n",
        "def save_to_excel(self, data, target_user):\n",
        "    \"\"\"分析結果をExcelファイルに保存する\"\"\"\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.title = \"分析結果\"\n",
        "\n",
        "    # ヘッダー行を作成\n",
        "    headers = [\n",
        "        'ユーザー名', 'スクリーンネーム', 'フォロワー数', 'フォロー数',\n",
        "        'ツイート数', 'リプライ数', '説明', '場所', 'URL',\n",
        "        '最新ツイート1', '最新ツイート2', '最新ツイート3',\n",
        "        '相手からのリプライ1', '相手からのリプライ日時1',\n",
        "        '相手からのリプライ2', '相手からのリプライ日時2',\n",
        "        '相手からのリプライ3', '相手からのリプライ日時3'\n",
        "    ]\n",
        "    sheet.append(headers)\n",
        "\n",
        "    # データを行として追加\n",
        "    for user_data in data:\n",
        "        profile = user_data['profile']\n",
        "        recent_tweets = user_data['recent_tweets']\n",
        "        received_replies = user_data.get('received_replies', [])\n",
        "\n",
        "        # ツイートとリプライの配列を3件に調整\n",
        "        tweets = (recent_tweets + [''] * 3)[:3]\n",
        "\n",
        "        # リプライデータの準備\n",
        "        reply_data = []\n",
        "        for reply in (received_replies + [{'text': '', 'created_at': ''}] * 3)[:3]:\n",
        "            reply_data.extend([reply['text'], reply['created_at']])\n",
        "\n",
        "        row = [\n",
        "            profile['name'],\n",
        "            profile['screen_name'],\n",
        "            profile['followers_count'],\n",
        "            profile['friends_count'],\n",
        "            profile['statuses_count'],\n",
        "            user_data['reply_count'],\n",
        "            profile['description'],\n",
        "            profile.get('location', ''),\n",
        "            profile.get('url', ''),\n",
        "            *tweets,           # 最新の3件のツイート\n",
        "            *reply_data       # 相手からのリプライ3件分（テキストと日時）\n",
        "        ]\n",
        "        sheet.append(row)\n",
        "\n",
        "    # 列幅の自動調整\n",
        "    for column in sheet.columns:\n",
        "        max_length = 0\n",
        "        column_letter = get_column_letter(column[0].column)\n",
        "        for cell in column:\n",
        "            try:\n",
        "                if len(str(cell.value)) > max_length:\n",
        "                    max_length = len(str(cell.value))\n",
        "            except:\n",
        "                pass\n",
        "        adjusted_width = min(max_length + 2, 50)  # 最大幅を50文字に制限\n",
        "        sheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "    # ファイルを保存\n",
        "    filename = f\"リプライ分析_{target_user}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
        "    workbook.save(filename)\n",
        "    print(f\"Excelファイルを保存しました: {filename}\")\n",
        "\n",
        "# Google Colab用の実行コード\n",
        "output.enable_custom_widget_manager()  # Google Colabでカスタムウィジェットを有効にする\n",
        "\n",
        "# 分析の実行\n",
        "await run_analysis()"
      ],
      "metadata": {
        "id": "WguCgLKhbBeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b91d89a-a794-4f11-83e2-be94a92a5cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "認証に成功しました！\n",
            "分析対象のユーザー名を入力してください（@を除く）: sora19ai\n",
            "最小リプライ数を入力してください（例: 3）: 2\n",
            "分析するツイート数を入力してください（例: 200）: 200\n",
            "\n",
            "sora19aiのリプライを分析します...\n",
            "- 分析対象ツイート数: 200\n",
            "- 最小リプライ数: 2\n",
            "sora19aiのツイートを分析中...\n",
            "ツイートを取得中...\n",
            "ユーザー 1108k4xi の情報を取得しました\n",
            "20件のツイートを分析済み\n",
            "ユーザー AquaCleanWater の情報を取得しました\n",
            "ユーザー kawa11japan の情報を取得しました\n",
            "40件のツイートを分析済み\n",
            "60件のツイートを分析済み\n",
            "80件のツイートを分析済み\n",
            "ユーザー solt152152 の情報を取得しました\n",
            "100件のツイートを分析済み\n",
            "120件のツイートを分析済み\n",
            "140件のツイートを分析済み\n",
            "160件のツイートを分析済み\n",
            "180件のツイートを分析済み\n",
            "200件のツイートを分析済み\n",
            "\n",
            "分析完了: 209件のツイートを処理\n",
            "リプライ先ユーザー数: 4人\n",
            "\n",
            "リプライの多いユーザーの情報を収集中...\n",
            "@1108k4xiの情報を取得しました（リプライ数: 2）\n",
            "分析結果を保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/analysis_sora19ai_20241229_220205.json\n",
            "\n",
            "Excelファイルを保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/リプライ分析_sora19ai_20241229_220205.xlsx\n",
            "\n",
            "分析が完了しました！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 非同期で分析を実行する関数\n",
        "async def run_analysis():\n",
        "        # TwitterProfileAnalyzerクラスのインスタンスを作成\n",
        "        analyzer = TwitterProfileAnalyzer()\n",
        "\n",
        "        # セットアップを実行し、認証が成功したか確認\n",
        "        if not await analyzer.setup():\n",
        "            return  # 認証に失敗した場合は関数を終了\n",
        "\n",
        "        # ユーザー入力を対話的に取得\n",
        "        target_user = input(\"分析対象のユーザー名を入力してください（@を除く）: \")  # 分析対象のTwitterユーザー名を入力\n",
        "        min_replies = int(input(\"最小リプライ数を入力してください（例: 3）: \"))  # リプライの最小数を入力\n",
        "        tweets_to_analyze = int(input(\"分析するツイート数を入力してください（例: 200）: \"))  # 分析するツイートの数を入力\n",
        "\n",
        "        # 分析の開始をユーザーに通知\n",
        "        print(f\"\\n{target_user}のリプライを分析します...\")\n",
        "        print(f\"- 分析対象ツイート数: {tweets_to_analyze}\")\n",
        "        print(f\"- 最小リプライ数: {min_replies}\")\n",
        "\n",
        "        # 指定したユーザーのリプライを分析\n",
        "        reply_counter, reply_users = await analyzer.analyze_user_replies(target_user, tweets_to_analyze)\n",
        "\n",
        "        # リプライが見つからなかった場合の処理\n",
        "        if not reply_counter:\n",
        "            print(\"\\nリプライが見つかりませんでした。\")\n",
        "            return  # 関数を終了\n",
        "\n",
        "        # 頻繁にリプライしているユーザーの情報を収集するためのリストを初期化\n",
        "        frequent_repliers_data = []\n",
        "        print(\"\\nリプライの多いユーザーの情報を収集中...\")\n",
        "\n",
        "        # リプライカウントが多い順にユーザーを処理\n",
        "        for screen_name, reply_count in reply_counter.most_common():\n",
        "            # 最小リプライ数以上かつユーザー情報が存在する場合に処理\n",
        "            if reply_count >= min_replies and screen_name in reply_users:\n",
        "                try:\n",
        "                    # リプライ先のユーザーオブジェクトを取得\n",
        "                    user = reply_users[screen_name]\n",
        "                    # ユーザーのプロフィール情報を取得\n",
        "                    profile_data = await analyzer.get_user_profile(user)\n",
        "                    # ユーザーの最新3件のツイートを取得\n",
        "                    tweets = await analyzer.get_user_tweets(user, count=3)\n",
        "\n",
        "                    # 相手からのリプライを取得（Twikitを使用）\n",
        "                    received_replies = []\n",
        "                    try:\n",
        "                        # 検索クエリを構成\n",
        "                        query = f\"from:{screen_name} to:{target_user}\"\n",
        "                        # Twikitのsearch_tweet関数を使用\n",
        "                        search_results = await analyzer.client.search_tweet(query, \"Latest\", count=20)\n",
        "\n",
        "                        for tweet in search_results:\n",
        "                            # ツイートがリプライであることを確認\n",
        "                            tweet_data = tweet.__dict__.get('data', {})\n",
        "                            referenced_tweets = tweet_data.get('referenced_tweets', [])\n",
        "\n",
        "                            is_reply = any(ref.get('type') == 'replied_to' for ref in referenced_tweets)\n",
        "                            if is_reply:\n",
        "                                received_replies.append({\n",
        "                                    'text': tweet.text,\n",
        "                                    'created_at': tweet.created_at,\n",
        "                                    'id': tweet.id\n",
        "                                })\n",
        "\n",
        "                            if len(received_replies) >= 3:  # 最大3件まで取得\n",
        "                                break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"リプライの取得中にエラーが発生: {e}\")\n",
        "                        received_replies = []\n",
        "\n",
        "                    # プロフィール情報が取得できた場合にデータを構築\n",
        "                    if profile_data:\n",
        "                        user_data = {\n",
        "                            'profile': profile_data,            # ユーザープロフィール情報\n",
        "                            'reply_count': reply_count,         # リプライ数\n",
        "                            'recent_tweets': tweets,            # 最近のツイート\n",
        "                            'received_replies': received_replies  # 相手からのリプライ（新機能）\n",
        "                        }\n",
        "                        # 頻繁なリプライユーザーのリストに追加\n",
        "                        frequent_repliers_data.append(user_data)\n",
        "                        # 情報取得完了を通知\n",
        "                        print(f\"@{screen_name}の情報を取得しました（リプライ数: {reply_count}）\")\n",
        "                except Exception as e:\n",
        "                    # ユーザー情報の取得に失敗した場合のエラーメッセージ\n",
        "                    print(f\"@{screen_name}の情報取得に失敗: {e}\")\n",
        "                    continue  # 次のユーザーに進む\n",
        "\n",
        "        # 収集したデータが存在する場合に結果を保存\n",
        "        if frequent_repliers_data:\n",
        "            # JSONファイルとして保存\n",
        "            analyzer.save_results(frequent_repliers_data, target_user)\n",
        "            # Excelファイルとして保存\n",
        "            analyzer.save_to_excel(frequent_repliers_data, target_user)\n",
        "            # 分析完了を通知\n",
        "            print(\"\\n分析が完了しました！\")\n",
        "        else:\n",
        "            # 分析対象のユーザーが見つからなかった場合のメッセージ\n",
        "            print(\"\\n分析対象となるユーザーが見つかりませんでした。\")\n",
        "\n",
        "# Excelファイルに保存する関数\n",
        "def save_to_excel(self, data, target_user):\n",
        "    \"\"\"分析結果をExcelファイルに保存する\"\"\"\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.title = \"分析結果\"\n",
        "\n",
        "    # ヘッダー行を作成\n",
        "    headers = [\n",
        "        'ユーザー名', 'スクリーンネーム', 'フォロワー数', 'フォロー数',\n",
        "        'ツイート数', 'リプライ数', '説明', '場所', 'URL',\n",
        "        '最新ツイート1', '最新ツイート2', '最新ツイート3',\n",
        "        '相手からのリプライ1', '相手からのリプライ日時1',\n",
        "        '相手からのリプライ2', '相手からのリプライ日時2',\n",
        "        '相手からのリプライ3', '相手からのリプライ日時3'\n",
        "    ]\n",
        "    sheet.append(headers)\n",
        "\n",
        "    # データを行として追加\n",
        "    for user_data in data:\n",
        "        profile = user_data['profile']\n",
        "        recent_tweets = user_data['recent_tweets']\n",
        "        received_replies = user_data.get('received_replies', [])\n",
        "\n",
        "        # ツイートとリプライの配列を3件に調整\n",
        "        tweets = (recent_tweets + [''] * 3)[:3]\n",
        "\n",
        "        # リプライデータの準備\n",
        "        reply_data = []\n",
        "        for reply in (received_replies + [{'text': '', 'created_at': ''}] * 3)[:3]:\n",
        "            reply_data.extend([reply['text'], reply['created_at']])\n",
        "\n",
        "        row = [\n",
        "            profile['name'],\n",
        "            profile['screen_name'],\n",
        "            profile['followers_count'],\n",
        "            profile['friends_count'],\n",
        "            profile['statuses_count'],\n",
        "            user_data['reply_count'],\n",
        "            profile['description'],\n",
        "            profile.get('location', ''),\n",
        "            profile.get('url', ''),\n",
        "            *tweets,           # 最新の3件のツイート\n",
        "            *reply_data       # 相手からのリプライ3件分（テキストと日時）\n",
        "        ]\n",
        "        sheet.append(row)\n",
        "\n",
        "    # 列幅の自動調整\n",
        "    for column in sheet.columns:\n",
        "        max_length = 0\n",
        "        column_letter = get_column_letter(column[0].column)\n",
        "        for cell in column:\n",
        "            try:\n",
        "                if len(str(cell.value)) > max_length:\n",
        "                    max_length = len(str(cell.value))\n",
        "            except:\n",
        "                pass\n",
        "        adjusted_width = min(max_length + 2, 50)  # 最大幅を50文字に制限\n",
        "        sheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "    # ファイルを保存\n",
        "    filename = f\"リプライ分析_{target_user}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
        "    workbook.save(filename)\n",
        "    print(f\"Excelファイルを保存しました: {filename}\")\n",
        "\n",
        "# Google Colab用の実行コード\n",
        "output.enable_custom_widget_manager()  # Google Colabでカスタムウィジェットを有効にする\n",
        "\n",
        "# 分析の実行\n",
        "await run_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO-VTUFRc1uU",
        "outputId": "9146ad53-7cbd-45b1-abae-bd0fe73306d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "認証に成功しました！\n",
            "分析対象のユーザー名を入力してください（@を除く）: sora19ai\n",
            "最小リプライ数を入力してください（例: 3）: 2\n",
            "分析するツイート数を入力してください（例: 200）: 200\n",
            "\n",
            "sora19aiのリプライを分析します...\n",
            "- 分析対象ツイート数: 200\n",
            "- 最小リプライ数: 2\n",
            "sora19aiのツイートを分析中...\n",
            "ツイートを取得中...\n",
            "ユーザー 1108k4xi の情報を取得しました\n",
            "20件のツイートを分析済み\n",
            "ユーザー AquaCleanWater の情報を取得しました\n",
            "ユーザー kawa11japan の情報を取得しました\n",
            "40件のツイートを分析済み\n",
            "60件のツイートを分析済み\n",
            "80件のツイートを分析済み\n",
            "ユーザー solt152152 の情報を取得しました\n",
            "100件のツイートを分析済み\n",
            "120件のツイートを分析済み\n",
            "140件のツイートを分析済み\n",
            "160件のツイートを分析済み\n",
            "180件のツイートを分析済み\n",
            "200件のツイートを分析済み\n",
            "\n",
            "分析完了: 209件のツイートを処理\n",
            "リプライ先ユーザー数: 4人\n",
            "\n",
            "リプライの多いユーザーの情報を収集中...\n",
            "@1108k4xiの情報を取得しました（リプライ数: 2）\n",
            "分析結果を保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/analysis_sora19ai_20241229_220527.json\n",
            "\n",
            "Excelファイルを保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/リプライ分析_sora19ai_20241229_220527.xlsx\n",
            "\n",
            "分析が完了しました！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "相手からのリプライ数　３回"
      ],
      "metadata": {
        "id": "YsksCDpzdxeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "\n",
        "# 非同期で分析を実行する関数\n",
        "async def run_analysis():\n",
        "        # TwitterProfileAnalyzerクラスのインスタンスを作成\n",
        "        analyzer = TwitterProfileAnalyzer()\n",
        "\n",
        "        # セットアップを実行し、認証が成功したか確認\n",
        "        if not await analyzer.setup():\n",
        "            return  # 認証に失敗した場合は関数を終了\n",
        "\n",
        "        # ユーザー入力を対話的に取得\n",
        "        target_user = input(\"分析対象のユーザー名を入力してください（@を除く）: \")\n",
        "        min_replies = int(input(\"最小リプライ数を入力してください（例: 3）: \"))\n",
        "        tweets_to_analyze = int(input(\"分析するツイート数を入力してください（例: 200）: \"))\n",
        "\n",
        "        print(f\"\\n{target_user}へのリプライを分析します...\")\n",
        "        print(f\"- 分析対象ツイート数: {tweets_to_analyze}\")\n",
        "        print(f\"- 最小リプライ数: {min_replies}\")\n",
        "\n",
        "        # 相手からのリプライを分析\n",
        "        reply_counter = Counter()  # リプライ数をカウントするカウンター\n",
        "        reply_users = {}  # リプライしてきたユーザーの情報を保持\n",
        "        reply_contents = defaultdict(list)  # ユーザーごとのリプライ内容を保持\n",
        "\n",
        "        try:\n",
        "            # 過去のツイートを検索して相手からのリプライを取得\n",
        "            query = f\"to:{target_user}\"\n",
        "            search_results = await analyzer.client.search_tweet(query, \"Latest\", count=100)\n",
        "\n",
        "            for tweet in search_results:\n",
        "                try:\n",
        "                    author = tweet.user.screen_name\n",
        "                    # リプライ数をカウント\n",
        "                    reply_counter[author] += 1\n",
        "                    # ユーザー情報を保存\n",
        "                    if author not in reply_users:\n",
        "                        reply_users[author] = tweet.user\n",
        "                    # リプライ内容を保存（最大3件まで）\n",
        "                    if len(reply_contents[author]) < 3:\n",
        "                        reply_contents[author].append({\n",
        "                            'text': tweet.text,\n",
        "                            'created_at': str(tweet.created_at),\n",
        "                            'id': tweet.id\n",
        "                        })\n",
        "                except AttributeError:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"リプライの検索中にエラー: {e}\")\n",
        "            return\n",
        "\n",
        "        # リプライが見つからなかった場合の処理\n",
        "        if not reply_counter:\n",
        "            print(\"\\nリプライが見つかりませんでした。\")\n",
        "            return\n",
        "\n",
        "        # 頻繁にリプライしているユーザーの情報を収集\n",
        "        frequent_repliers_data = []\n",
        "        print(\"\\nリプライの多いユーザーの情報を収集中...\")\n",
        "\n",
        "        # リプライカウントが多い順にユーザーを処理\n",
        "        for screen_name, reply_count in reply_counter.most_common():\n",
        "            if reply_count >= min_replies:\n",
        "                try:\n",
        "                    user = reply_users[screen_name]\n",
        "                    # ユーザーのプロフィール情報を取得\n",
        "                    profile_data = await analyzer.get_user_profile(user)\n",
        "                    # 最新のツイートを取得\n",
        "                    tweets = await analyzer.get_user_tweets(user, count=3)\n",
        "\n",
        "                    if profile_data:\n",
        "                        user_data = {\n",
        "                            'profile': profile_data,\n",
        "                            'reply_count': reply_count,  # 相手からのリプライ数\n",
        "                            'recent_tweets': tweets,\n",
        "                            'received_replies': reply_contents[screen_name]  # 保存したリプライ内容\n",
        "                        }\n",
        "                        frequent_repliers_data.append(user_data)\n",
        "                        print(f\"@{screen_name}の情報を取得しました（リプライ数: {reply_count}）\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"@{screen_name}の情報取得に失敗: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # 収集したデータが存在する場合に結果を保存\n",
        "        if frequent_repliers_data:\n",
        "            analyzer.save_results(frequent_repliers_data, target_user)\n",
        "            analyzer.save_to_excel(frequent_repliers_data, target_user)\n",
        "            print(\"\\n分析が完了しました！\")\n",
        "        else:\n",
        "            print(\"\\n分析対象となるユーザーが見つかりませんでした。\")\n",
        "\n",
        "def save_to_excel(self, data, target_user):\n",
        "    \"\"\"分析結果をExcelファイルに保存する\"\"\"\n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    sheet.title = \"分析結果\"\n",
        "\n",
        "    headers = [\n",
        "        'ユーザー名', 'スクリーンネーム', 'フォロワー数', 'フォロー数',\n",
        "        'ツイート数', '相手からのリプライ数', '説明', '場所', 'URL',\n",
        "        '最新ツイート1', '最新ツイート2', '最新ツイート3',\n",
        "        '相手からのリプライ1', '相手からのリプライ日時1',\n",
        "        '相手からのリプライ2', '相手からのリプライ日時2',\n",
        "        '相手からのリプライ3', '相手からのリプライ日時3'\n",
        "    ]\n",
        "    sheet.append(headers)\n",
        "\n",
        "    for user_data in data:\n",
        "        profile = user_data['profile']\n",
        "        recent_tweets = [tweet.text if hasattr(tweet, 'text') else str(tweet) for tweet in user_data['recent_tweets']]\n",
        "        received_replies = user_data.get('received_replies', [])\n",
        "\n",
        "        tweets = (recent_tweets + [''] * 3)[:3]\n",
        "\n",
        "        reply_data = []\n",
        "        for reply in (received_replies + [{'text': '', 'created_at': ''}] * 3)[:3]:\n",
        "            reply_data.extend([reply['text'], reply['created_at']])\n",
        "\n",
        "        row = [\n",
        "            profile['name'],\n",
        "            profile['screen_name'],\n",
        "            profile['followers_count'],\n",
        "            profile['friends_count'],\n",
        "            profile['statuses_count'],\n",
        "            user_data['reply_count'],  # 相手からのリプライ数\n",
        "            profile['description'],\n",
        "            profile.get('location', ''),\n",
        "            profile.get('url', ''),\n",
        "            *tweets,\n",
        "            *reply_data\n",
        "        ]\n",
        "        sheet.append(row)\n",
        "\n",
        "    for column in sheet.columns:\n",
        "        max_length = 0\n",
        "        column_letter = get_column_letter(column[0].column)\n",
        "        for cell in column:\n",
        "            try:\n",
        "                if len(str(cell.value)) > max_length:\n",
        "                    max_length = len(str(cell.value))\n",
        "            except:\n",
        "                pass\n",
        "        adjusted_width = min(max_length + 2, 50)\n",
        "        sheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "    filename = f\"リプライ分析_{target_user}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
        "    save_path = os.path.join(os.getcwd(), filename)\n",
        "    workbook.save(save_path)\n",
        "    print(f\"Excelファイルを保存しました: {save_path}\")\n",
        "\n",
        "await run_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dRB53Vcdzp1",
        "outputId": "a28e7006-fdd5-47e1-843a-a9adf2be2272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "認証に成功しました！\n",
            "分析対象のユーザー名を入力してください（@を除く）: sora19ai\n",
            "最小リプライ数を入力してください（例: 3）: 2\n",
            "分析するツイート数を入力してください（例: 200）: 200\n",
            "\n",
            "sora19aiへのリプライを分析します...\n",
            "- 分析対象ツイート数: 200\n",
            "- 最小リプライ数: 2\n",
            "\n",
            "リプライの多いユーザーの情報を収集中...\n",
            "@sora19aiの情報を取得しました（リプライ数: 9）\n",
            "@seikimitsumoriの情報を取得しました（リプライ数: 2）\n",
            "分析結果を保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/analysis_sora19ai_20241229_221050.json\n",
            "\n",
            "Excelファイルを保存しました: /content/drive/MyDrive/Twitter_Analysis/profile_results/リプライ分析_sora19ai_20241229_221050.xlsx\n",
            "\n",
            "分析が完了しました！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "テスト"
      ],
      "metadata": {
        "id": "zlQKk2rOlaL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# キーワード検索"
      ],
      "metadata": {
        "id": "YB_cBd-N9JD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yDbOGVX0lZJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "キーワード検索　初期設定のセル"
      ],
      "metadata": {
        "id": "3bIu7lYW25-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインストール\n",
        "!pip install twikit pandas openpyxl\n",
        "\n",
        "# 必要なインポート\n",
        "from twikit import Client\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "# Google Driveのマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ディレクトリ設定\n",
        "WORK_DIR = '/content/drive/MyDrive/Twitter_Analysis'\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "COOKIE_DIR = f\"{WORK_DIR}/twitter_json\"\n",
        "os.makedirs(COOKIE_DIR, exist_ok=True)\n",
        "\n",
        "RESULTS_DIR = f\"{WORK_DIR}/keyword_search_results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"ディレクトリ設定完了！\")\n",
        "print(f\"作業ディレクトリ: {WORK_DIR}\")\n",
        "print(f\"クッキー保存先: {COOKIE_DIR}\")\n",
        "print(f\"結果保存先: {RESULTS_DIR}\")\n",
        "\n",
        "# クッキーファイルのアップロード\n",
        "def upload_cookie_file():\n",
        "    print(\"cookie_edit.jsonファイルをアップロードしてください\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if 'cookie_edit.json' in uploaded:\n",
        "        with open(f\"{COOKIE_DIR}/cookie_edit.json\", 'wb') as f:\n",
        "            f.write(uploaded['cookie_edit.json'])\n",
        "        print(\"クッキーファイルを保存しました\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"cookie_edit.jsonファイルがアップロードされませんでした\")\n",
        "        return False\n",
        "\n",
        "upload_cookie_file()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "-PniB1PW27Lo",
        "outputId": "9c740872-72bd-4dce-ddfb-159706958664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twikit in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: httpx[socks] in /usr/local/lib/python3.10/dist-packages (from twikit) (0.28.1)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from twikit) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from twikit) (4.12.3)\n",
            "Requirement already satisfied: pyotp in /usr/local/lib/python3.10/dist-packages (from twikit) (2.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from twikit) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->twikit) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (3.10)\n",
            "Requirement already satisfied: socksio==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[socks]->twikit) (1.0.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[socks]->twikit) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[socks]->twikit) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[socks]->twikit) (1.2.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ディレクトリ設定完了！\n",
            "作業ディレクトリ: /content/drive/MyDrive/Twitter_Analysis\n",
            "クッキー保存先: /content/drive/MyDrive/Twitter_Analysis/twitter_json\n",
            "結果保存先: /content/drive/MyDrive/Twitter_Analysis/keyword_search_results\n",
            "cookie_edit.jsonファイルをアップロードしてください\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-93762caa-9b46-4c4e-8f9b-67c3efeeafe6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-93762caa-9b46-4c4e-8f9b-67c3efeeafe6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cookie_edit.json to cookie_edit (1).json\n",
            "cookie_edit.jsonファイルがアップロードされませんでした\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "キーワード検索　クラス定義セル"
      ],
      "metadata": {
        "id": "XISJ7dj929To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterKeywordAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.client = Client(language='en-US')\n",
        "        self.cookie_path = f\"{COOKIE_DIR}/cookie_edit.json\"\n",
        "        self.results_dir = RESULTS_DIR\n",
        "\n",
        "    async def setup(self):\n",
        "        try:\n",
        "            with open(self.cookie_path, 'r', encoding='utf-8') as file:\n",
        "                cookies = json.load(file)\n",
        "            self.client.set_cookies(cookies)\n",
        "            print(\"認証に成功しました！\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"認証エラー: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def search_with_keyword(self, keyword, count=20, sort_by='latest'):\n",
        "        \"\"\"キーワードを含むツイートを検索し、関連情報を取得する\"\"\"\n",
        "        try:\n",
        "            search_results = []\n",
        "            print(f\"'{keyword}' に関連する情報を検索中...(並び順: {sort_by})\")\n",
        "\n",
        "            product_type = 'Latest' if sort_by == 'latest' else 'Top'\n",
        "            results = await self.client.search_tweet(\n",
        "                query=keyword,\n",
        "                product=product_type,\n",
        "                count=min(count, 20)\n",
        "            )\n",
        "\n",
        "            for tweet in results:\n",
        "                user_data = {\n",
        "                    'user_id': tweet.user.id,\n",
        "                    'name': tweet.user.name,\n",
        "                    'screen_name': tweet.user.screen_name,\n",
        "                    'profile_description': tweet.user.description,\n",
        "                    'profile_url': f\"https://twitter.com/{tweet.user.screen_name}\",\n",
        "                    'followers_count': tweet.user.followers_count,\n",
        "                    'following_count': tweet.user.following_count,\n",
        "                    'profile_image_url': tweet.user.profile_image_url,\n",
        "                    'location': tweet.user.location\n",
        "                }\n",
        "\n",
        "                tweet_data = {\n",
        "                    'tweet_id': tweet.id,\n",
        "                    'tweet_url': f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\",\n",
        "                    'text': tweet.text,\n",
        "                    'created_at': tweet.created_at,\n",
        "                    'retweet_count': tweet.retweet_count,\n",
        "                    'like_count': tweet.favorite_count,\n",
        "                    'reply_count': tweet.reply_count if hasattr(tweet, 'reply_count') else 0,\n",
        "                    'is_retweet': bool(tweet.retweeted_tweet),\n",
        "                    'is_quote': tweet.is_quote_status,\n",
        "                    'language': tweet.lang\n",
        "                }\n",
        "\n",
        "                keyword_locations = []\n",
        "                if keyword.lower() in tweet.text.lower():\n",
        "                    keyword_locations.append('tweet_text')\n",
        "                if keyword.lower() in tweet.user.description.lower():\n",
        "                    keyword_locations.append('profile_description')\n",
        "                if keyword.lower() in tweet.user.name.lower():\n",
        "                    keyword_locations.append('user_name')\n",
        "                if keyword.lower() in tweet.user.screen_name.lower():\n",
        "                    keyword_locations.append('screen_name')\n",
        "\n",
        "                search_results.append({\n",
        "                    'user': user_data,\n",
        "                    'tweet': tweet_data,\n",
        "                    'keyword_locations': keyword_locations\n",
        "                })\n",
        "\n",
        "            if sort_by == 'likes':\n",
        "                search_results.sort(key=lambda x: x['tweet']['like_count'], reverse=True)\n",
        "\n",
        "            return search_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"検索エラー: {e}\")\n",
        "            return []\n",
        "\n",
        "    # save_results と save_to_excel メソッドはそのまま\n",
        "    def save_results(self, results, filename_prefix):\n",
        "        \"\"\"検索結果をJSONファイルとして保存する\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"{self.results_dir}/{filename_prefix}_{timestamp}.json\"\n",
        "\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as file:\n",
        "                json.dump(results, file, ensure_ascii=False, indent=2, default=str)\n",
        "            print(f\"\\n結果を保存しました: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"結果の保存中にエラーが発生しました: {e}\")\n",
        "\n",
        "    def save_to_excel(self, results, keyword, sort_by):\n",
        "        \"\"\"検索結果をExcelファイルとして保存\"\"\"\n",
        "        try:\n",
        "            # 結果をDataFrame用に整形\n",
        "            rows = []\n",
        "            for result in results:\n",
        "                user = result['user']\n",
        "                tweet = result['tweet']\n",
        "                locations = result['keyword_locations']\n",
        "\n",
        "                row = {\n",
        "                    '検索日時': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    '検索キーワード': keyword,\n",
        "                    '並び順': sort_by,\n",
        "                    '投稿日時': tweet['created_at'],\n",
        "                    'アカウント名': user['name'],\n",
        "                    'ユーザーID': f\"@{user['screen_name']}\",\n",
        "                    'プロフィール文': user['profile_description'],\n",
        "                    'フォロワー数': user['followers_count'],\n",
        "                    'フォロー数': user['following_count'],\n",
        "                    'ツイート本文': tweet['text'],\n",
        "                    'いいね数': tweet['like_count'],\n",
        "                    'リツイート数': tweet['retweet_count'],\n",
        "                    'リプライ数': tweet['reply_count'],\n",
        "                    'ツイートURL': tweet['tweet_url'],\n",
        "                    'アカウントURL': user['profile_url'],\n",
        "                    'キーワード出現場所': ', '.join(locations),\n",
        "                    '場所': user['location'] if user['location'] else '',\n",
        "                    '言語': tweet['language']\n",
        "                }\n",
        "                rows.append(row)\n",
        "\n",
        "            # DataFrameを作成\n",
        "            df = pd.DataFrame(rows)\n",
        "\n",
        "            # Excelファイル名を生成\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            excel_file = f\"{self.results_dir}/Twitter検索結果_{keyword}_{sort_by}_{timestamp}.xlsx\"\n",
        "\n",
        "            # Excelファイルとして保存\n",
        "            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "                df.to_excel(writer, sheet_name='検索結果', index=False)\n",
        "\n",
        "                # 列幅の自動調整\n",
        "                worksheet = writer.sheets['検索結果']\n",
        "                for idx, col in enumerate(df.columns):\n",
        "                    max_length = max(\n",
        "                        df[col].astype(str).apply(len).max(),\n",
        "                        len(str(col))\n",
        "                    )\n",
        "                    # 最大幅を50文字に制限\n",
        "                    adjusted_width = min(max_length + 2, 50)\n",
        "                    worksheet.column_dimensions[chr(65 + idx)].width = adjusted_width\n",
        "\n",
        "            print(f\"\\nExcelファイルを保存しました: {excel_file}\")\n",
        "            return excel_file\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Excelファイルの保存中にエラーが発生しました: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "qWg0x-o_2-cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "キーワード検索　実行セル"
      ],
      "metadata": {
        "id": "gyaD1s693ByO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_search():\n",
        "    analyzer = TwitterKeywordAnalyzer()\n",
        "\n",
        "    if not await analyzer.setup():\n",
        "        return\n",
        "\n",
        "    # 検索設定を対話的に取得\n",
        "    keyword = input(\"検索キーワードを入力してください: \")\n",
        "    count = int(input(\"取得する結果の数を入力してください（最大20）: \"))\n",
        "\n",
        "    print(\"\\n検索オプション:\")\n",
        "    print(\"1: 新しい順\")\n",
        "    print(\"2: 人気順\")\n",
        "    print(\"3: いいね数順\")\n",
        "\n",
        "    try:\n",
        "        option = int(input(\"検索オプションを選択してください (1-3): \"))\n",
        "        sort_by = {\n",
        "            1: 'latest',\n",
        "            2: 'top',\n",
        "            3: 'likes'\n",
        "        }.get(option, 'latest')\n",
        "    except ValueError:\n",
        "        print(\"無効な入力です。デフォルトの'新しい順'で検索します。\")\n",
        "        sort_by = 'latest'\n",
        "\n",
        "    results = await analyzer.search_with_keyword(keyword, count, sort_by)\n",
        "\n",
        "    if not results:\n",
        "        print(\"検索結果が見つかりませんでした。\")\n",
        "        return\n",
        "\n",
        "    sort_type = {\n",
        "        'latest': '新しい順',\n",
        "        'top': '人気順',\n",
        "        'likes': 'いいね数順'\n",
        "    }[sort_by]\n",
        "\n",
        "    print(f\"\\n検索結果 ({len(results)} 件) - {sort_type}:\")\n",
        "    for result in results:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        user = result['user']\n",
        "        tweet = result['tweet']\n",
        "\n",
        "        print(f\"投稿日時: {tweet['created_at']}\")\n",
        "        print(f\"いいね数: {tweet['like_count']}\")\n",
        "        print(f\"\\nユーザー情報: {user['name']} (@{user['screen_name']})\")\n",
        "        print(f\"ツイート: {tweet['text']}\")\n",
        "        print(f\"URL: {tweet['tweet_url']}\")\n",
        "\n",
        "    if results:\n",
        "        analyzer.save_results(results, f\"{keyword}_{sort_by}\")\n",
        "        analyzer.save_to_excel(results, keyword, sort_type)\n",
        "\n",
        "# Google Colab用の実行\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# 分析実行\n",
        "await run_search()"
      ],
      "metadata": {
        "id": "B9PZA9LW3BGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2cde64-ae0e-402e-c992-5ed442848b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "認証に成功しました！\n",
            "検索キーワードを入力してください: dify\n",
            "取得する結果の数を入力してください（最大20）: 20\n",
            "\n",
            "検索オプション:\n",
            "1: 新しい順\n",
            "2: 人気順\n",
            "3: いいね数順\n",
            "検索オプションを選択してください (1-3): 2\n",
            "'dify' に関連する情報を検索中...(並び順: top)\n",
            "\n",
            "検索結果 (19 件) - 人気順:\n",
            "\n",
            "==================================================\n",
            "投稿日時: Wed Dec 11 03:06:26 +0000 2024\n",
            "いいね数: 6\n",
            "\n",
            "ユーザー情報: katoshin (@katoshin_ii)\n",
            "ツイート: 要件定義を対話型で実現するDifyのチャットフローがなかなか大作になってしまった。 https://t.co/JKJNpFFn4A\n",
            "URL: https://twitter.com/katoshin_ii/status/1866680917722632626\n",
            "\n",
            "==================================================\n",
            "投稿日時: Sat Dec 07 07:53:30 +0000 2024\n",
            "いいね数: 171\n",
            "\n",
            "ユーザー情報: Tom | ドバイで生成AIやってる人 (@0x__tom)\n",
            "ツイート: Difyを使えば、「食事や体重の記録」から「記録に基づいたアドバイスの生成」までを一瞬で自動化できる。写真をアップするだけで、カロリー計算や体重をスプシに記録し、AIがデータを分析してパーソナライズされたアドバイスを提案。面倒な記録の手間を省き、健康管理をサポートしてくれます。 https://t.co/3YkdhDSbAb\n",
            "URL: https://twitter.com/0x__tom/status/1865303606683164768\n",
            "\n",
            "==================================================\n",
            "投稿日時: Wed Dec 11 16:35:55 +0000 2024\n",
            "いいね数: 5\n",
            "\n",
            "ユーザー情報: 森本洋平@すもちゃん| 動画×AI・Dify開発 (@sumo_wonderful)\n",
            "ツイート: Soraの記念すべき初体験AI生成は「sumo wrestler」と入力してストーリーもAIに自動作成してもらった。\n",
            "\n",
            "いくらすごいAI動画生成ツールでも\"相撲\"の学習量と知名度が少なすぎるんだろうね。\n",
            "AIツールを最初に試す時、相撲関連のプロンプト当てると大体そのAIツールの限界が見える。 https://t.co/4WiuQOHgNC\n",
            "URL: https://twitter.com/sumo_wonderful/status/1866884630345027681\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 13:44:45 +0000 2024\n",
            "いいね数: 6\n",
            "\n",
            "ユーザー情報: こう 土木設計×AI・プログラミング (@kou_sunh)\n",
            "ツイート: Gemini 2.0 flashに、Difyで土木設計図面の知識をいれて、土木の掘削標準図（サンプル）寸法を読んでもらいました。\n",
            "\n",
            "他モデルとか知識追加前は、寸法読取にミスがあったので嬉しいです。ただ、それぞれの寸法の関係式はまだめちゃくちゃで、作れるようになったら本格的に活用できそうなんだけど・・ https://t.co/OHgBVnHauB\n",
            "URL: https://twitter.com/kou_sunh/status/1869015879910146350\n",
            "\n",
            "==================================================\n",
            "投稿日時: Mon Dec 16 09:55:46 +0000 2024\n",
            "いいね数: 526\n",
            "\n",
            "ユーザー情報: サクサク (@SakuSaku23TOP8)\n",
            "ツイート: 記事生成もサムネ作成もXへの投稿も全部自動化してみました！  \n",
            "ReplitでXのトレンドサーチ\n",
            " →Difyで記事生成、サムネ生成、投稿生成\n",
            " →Difyから全メディアに投稿\n",
            "してます。 https://t.co/F9C4Y2Kl4U\n",
            "URL: https://twitter.com/SakuSaku23TOP8/status/1868595868548673712\n",
            "\n",
            "==================================================\n",
            "投稿日時: Sat Dec 14 07:27:45 +0000 2024\n",
            "いいね数: 737\n",
            "\n",
            "ユーザー情報: Tom | ドバイで生成AIやってる人 (@0x__tom)\n",
            "ツイート: Difyを使えば新人教育を一瞬で自動化できる。企業の作業マニュアルが入っているGoogleドライブをRAG化し、チャットボットに搭載することで新入社員は会話をしながら定型業務の学習が可能に。AIを使えば既存社員の時間を奪わず新人教育を効率化できる。 https://t.co/52mLtfdfz5\n",
            "URL: https://twitter.com/0x__tom/status/1867833841416851840\n",
            "\n",
            "==================================================\n",
            "投稿日時: Thu Dec 12 22:00:03 +0000 2024\n",
            "いいね数: 1004\n",
            "\n",
            "ユーザー情報: あるる ChatGPT × AIツール (@chatgptair)\n",
            "ツイート: まるちゃん、まだChatGPTしか使ってないの？\n",
            "\n",
            "文章生成は「Gemini 2.0」\n",
            "スライド制作は「イルシル」\n",
            "Web検索は「Genspark」\n",
            "図解作成は「NapkinAI」\n",
            "議事録は「tl;dv」\n",
            "長文処理は「NotebookLM」\n",
            "システム開発は「v0」\n",
            "アプリ開発は「Dify」\n",
            "webサイト制作は「Create」\n",
            "\n",
            "がおすすめだよ。 https://t.co/iA9D5QUG8A\n",
            "URL: https://twitter.com/chatgptair/status/1867328590830067851\n",
            "\n",
            "==================================================\n",
            "投稿日時: Mon Dec 09 18:23:23 +0000 2024\n",
            "いいね数: 474\n",
            "\n",
            "ユーザー情報: そら ☁️ Dify 自動化オタク📱 (@sora19ai)\n",
            "ツイート: DifyでXのAPI無料で使えるの便利すぎてハゲそう。 https://t.co/gOrRJLkhfx\n",
            "URL: https://twitter.com/sora19ai/status/1866186898131345729\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 01:13:55 +0000 2024\n",
            "いいね数: 4\n",
            "\n",
            "ユーザー情報: しろ | SaaSとAIが得意なライター兼ディレクター (@siro3460)\n",
            "ツイート: Difyのツール増えてきた～ https://t.co/FUslmblUrk\n",
            "URL: https://twitter.com/siro3460/status/1868826929752838246\n",
            "\n",
            "==================================================\n",
            "投稿日時: Mon Dec 16 14:41:23 +0000 2024\n",
            "いいね数: 46\n",
            "\n",
            "ユーザー情報: 池田 朋弘 | ChatGPT最強の仕事術3万部突破！ (@pop_ikeda)\n",
            "ツイート: ⏲️Gemini 2.0 Flashは「早い、美味い」（そして多分安い）\n",
            "\n",
            "DifyがGemini2.0Flashに対応したので、早速Difyで作っているツール（AIいけとも）のLLMをGemini2.0に変えました！\n",
            "\n",
            "元々Gemini1.5Proを使っていたので比較したところ、\n",
            "\n",
            "・圧倒的に早い\n",
            "・内容の精度も高い\n",
            "\n",
            "ということで、良いことづくし。 https://t.co/RICTXzFtzD\n",
            "URL: https://twitter.com/pop_ikeda/status/1868667746176909739\n",
            "\n",
            "==================================================\n",
            "投稿日時: Mon Dec 16 06:35:23 +0000 2024\n",
            "いいね数: 302\n",
            "\n",
            "ユーザー情報: Taiyo | AIで遊ぶ大学生 (@taiyo_ai_gakuse)\n",
            "ツイート: 理論上はこれでサービス量産できますよね。\n",
            "\n",
            "- GEAR.indigoで要件定義生成\n",
            "- Google AI Studioで参考アプリの動作動画を投げてコード生成\n",
            "- Cursorで細かい修正\n",
            "- 神威/KAMUIでシステムの全体像を可視化\n",
            "- Difyで独自AIバックエンドを作成\n",
            "- Jinbaflowでアプリ間連系を高速実装\n",
            "- Stripe\n",
            "URL: https://twitter.com/taiyo_ai_gakuse/status/1868545438179303721\n",
            "\n",
            "==================================================\n",
            "投稿日時: Mon Dec 16 23:48:31 +0000 2024\n",
            "いいね数: 18\n",
            "\n",
            "ユーザー情報: sangmin.eth | Dify Ambassador (@gijigae)\n",
            "ツイート: . @Klarna 社（スウェーデンのフィンテック企業）は去年から採用を止めている。同社のCEO曰く、4500人だった授業員が今は3500人。離職者が出ても新しい人は雇わない。減った人件費の一部は現従業員への賞与として還元する。その結果、効率化をもたらすAIの導入に全社で取り組むようになったとのこと。 https://t.co/uHb5vSs5lK\n",
            "URL: https://twitter.com/gijigae/status/1868805435266146369\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 05:58:26 +0000 2024\n",
            "いいね数: 2\n",
            "\n",
            "ユーザー情報: タロ.AI (@OptionTrader25)\n",
            "ツイート: 紙の領収書をAIで楽に集計するアプリをあなたも完全無料で使えます。\n",
            "Notionで細かく説明し、動画で分かりやすく解説してます。\n",
            "質問があればお答えします。\n",
            "\n",
            "MotionとYoutubeはリプ欄に\n",
            "\n",
            "#Dify #Googleスプレッドシート #Googleドライブ #GAS (Google Apps Script)を利用してます。 https://t.co/pEmKO256yz\n",
            "URL: https://twitter.com/OptionTrader25/status/1868898527252037924\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 14:38:00 +0000 2024\n",
            "いいね数: 127\n",
            "\n",
            "ユーザー情報: ぶんかい@AIで遊ぶ人 (@bunkaich)\n",
            "ツイート: ん？まてよ・・・\n",
            "え、やばい、動いた！したらこれ無料だ！！\n",
            "\n",
            "固定ポストのNotionに書き込むDifyのワークフロー、これよく考えてみたら、LLM使ってないのでDify Cloud版の無料アカウントでも使い放題ですわ。（たぶん）\n",
            "\n",
            "これ結構、いや、すごいおもしろいかも。\n",
            "ちょっと長いんでリプに続けます↓\n",
            "URL: https://twitter.com/bunkaich/status/1869029284226207813\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 16:48:11 +0000 2024\n",
            "いいね数: 13\n",
            "\n",
            "ユーザー情報: そら ☁️ Dify 自動化オタク📱 (@sora19ai)\n",
            "ツイート: Cline × MCPサーバーの生成がヤバい。\n",
            "URL: https://twitter.com/sora19ai/status/1869062044106064164\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 10:28:53 +0000 2024\n",
            "いいね数: 12\n",
            "\n",
            "ユーザー情報: Dify Base| Difyの基礎から応用まで (@dify_base)\n",
            "ツイート: DifyとLangSmithを連携することで、Difyチャットボットの会話ログを収集/分析できます。\n",
            "\n",
            "Difyチャットボットを作って終わりではなく、LangSmithを使うことで、運用改善に繋がります。\n",
            "\n",
            "実際にDifyのチャットのログが自動で反映されている様子👇 https://t.co/SJklMR06Qa\n",
            "URL: https://twitter.com/dify_base/status/1868966590114603218\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 03:50:28 +0000 2024\n",
            "いいね数: 75\n",
            "\n",
            "ユーザー情報: ぶんかい@AIで遊ぶ人 (@bunkaich)\n",
            "ツイート: OpenAI Day 8は「ChatGPT search」のアプデ！！\n",
            "\n",
            "GPTs経由でリサーチレポートや対話内容をNotionに飛ばせようにしてみました。\n",
            "\n",
            "→ DifyのNotionへ保存するワークフローをAPI化\n",
            "→ GPTsのActionsに登録\n",
            "→ あらゆるトークで「＠」ツール呼び出し\n",
            "→Notionに保存\n",
            "ChatGPT利用が増えたので捗ります^^ https://t.co/1vKteHyWBw\n",
            "URL: https://twitter.com/bunkaich/status/1868866324438695971\n",
            "\n",
            "==================================================\n",
            "投稿日時: Tue Dec 17 15:05:49 +0000 2024\n",
            "いいね数: 144\n",
            "\n",
            "ユーザー情報: 九原客 (@9hills)\n",
            "ツイート: 国内大模型ToB典型项目案例，排除某低价厂商。\n",
            "\n",
            "- 复杂应用（如RAG）一般在30-50万。\n",
            "- 类 Dify 平台一般是50-100万\n",
            "- 人天成本3000-4000，这是能做了大模型应用开发的。\n",
            "- 线下培训3-4w一天。\n",
            "\n",
            "感觉很贵，但是换成从业人员工资成本就知道不挣钱。 https://t.co/ZrkPp5NDH5\n",
            "URL: https://twitter.com/9hills/status/1869036282976075869\n",
            "\n",
            "==================================================\n",
            "投稿日時: Sat Dec 07 23:23:11 +0000 2024\n",
            "いいね数: 227\n",
            "\n",
            "ユーザー情報: Guoqi Sun🦋@guoqi.dev (@sun0225SUN)\n",
            "ツイート: Dify 的网站做的太花了，本来想学习一下，然后发现竟然是用 Framer 画的 🙃\n",
            "\n",
            "设计师淘汰了赚差价的程序员，还原度确实高哈，确实好看，动画也很丝滑 🤡 https://t.co/GUxwWLXSst\n",
            "URL: https://twitter.com/sun0225SUN/status/1865537568911298743\n",
            "\n",
            "結果を保存しました: /content/drive/MyDrive/Twitter_Analysis/keyword_search_results/dify_top_20241218_021726.json\n",
            "\n",
            "Excelファイルを保存しました: /content/drive/MyDrive/Twitter_Analysis/keyword_search_results/Twitter検索結果_dify_人気順_20241218_021726.xlsx\n"
          ]
        }
      ]
    }
  ]
}